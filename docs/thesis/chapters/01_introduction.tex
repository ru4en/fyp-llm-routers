\chapter{Introduction}
\label{ch:into}

In the past few years the landscape of large language models has expanded dramatically, with many domain specific as well as general-purpose agents emerging across domains such as healthcare (like Med-PaLM 2 and BioGPT), coding (like CodeLlama and GitHub Copilot), and research (like Claude Opus and GPT-4o). Organisations that provide inference as a service now face complex trade-offs between cost, latency, and capability: for example, GPT-4.5 can cost up to \$75 per million tokens compared with just \$0.15 for gemini-2.5-flash\footnote{https://sanand0.github.io/llmpricing/}\footnote{https://artificialanalysis.ai/}. Although these models could be vastly different in terms of capability, the problem organisations face is determining \textit{when} to deploy premium models versus more cost-effective alternatives for a given task / prompt. This suggests a need for intelligent routing systems that can analyse incoming prompts and direct them to the most appropriate model based on task complexity, required capabilities, and cost considerations.

Inspired by lower level (transformer-embedded) "router" such as the one employed by mistral for their Mixtral (MoE) model the goal of this was to allow for a more distributed, higher level prompt based routing between a verity of models with varying levels of cost and complexity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem statement}
\label{sec:intro_prob_art}

The proliferation of large language models has created a complex ecosystem where selecting the optimal model for a given task has become increasingly challenging.

Existing multi-agent routing systems reveal several shortcomings. First, many current routers rely on \textbf{manual configuration}. For example, Both Open AI's as well as Open WebUI's chat interface require explicitly toggling of tools/selection of agents from users, and listing models to skip, on a per-chat basis. This manual toggling is brittle. Second, LLM-based routers can suffer from reasoning \textbf{inefficiencies}. Recent studies identify \textit{"underthinking"} (prematurely abandoning good reasoning paths) and \textit{"overthinking"} (generating excessive, unnecessary steps) in modern LLMs. For instance, Wang \textit{et al.} (2025) find that top reasoning models often switch thoughts too quickly â€“ an \textit{"underthinking"} effect that hurts accuracy\footnote{https://arxiv.org/pdf/2501.18585v1}. Conversely, Kumar \textit{et al.} demonstrate how even simple queries can be made to "overthink" (spending many tokens on irrelevant chains of thought) without improving answers\footnote{https://arxiv.org/pdf/2502.02542v1}. Both phenomena imply wasted tokens. Finally, prompt interpretation remains imperfect: ambiguous or poorly phrased queries may be misrouted or require multiple LLM calls to resolve intent, leading to inefficiency.

Organisations and users face several key problems:
\begin{enumerate}
    \item \textbf{Cost-Efficiency Trade-offs}: High-capability models like GPT-4o and Claude 3 Opus provide powerful capabilities but at significantly higher costs than simpler models. Without intelligent routing, organisations and users may unnecessarily infer to expensive models for tasks that could be adequately handled by more cost-effective alternatives.
    
    \item \textbf{Selection Complexity}: With the dawn of function calling and Multi-modal Context Processing (MCP), most chat systems offer numerous specialised tools and functions, but determining which tools are appropriate for a given query often requires manual specification by users or developers.
    
    \item \textbf{Computational Resource Allocation}: Indiscriminate routing of all queries to high-performance models can lead to inefficient resource allocation, increased latency, and higher operational costs for LLM providers and users.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Objectives}
\label{sec:intro_aims_obj}

The premise of this research is to investigate whether pre-existing Natural Language Inference models such as Facebook's bart-large-mnli could be used as drop-in replacements to perform automated model selection and tool selection. Furthermore, we will examine the effectiveness of fine-tuning existing NLI models with specialised datasets designed for routing tasks.

The specific research objectives include:
\begin{itemize}
    \item Creating a LLM Router library that can be deploy to existing systems with ease.
    \item Experimenting with Pretrained NLI models such as bart-large-mnli for both tool routing and model selection.
    \item Evaluating and assessing the accuracy the effectiveness using a set of prompts.
    \item Incorporate it with an existing Chatbot UI platform such as OpenWebUI.
\end{itemize}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Solution approach}
% \label{sec:intro_sol} % label of Org section
% Briefly describe the solution approach and the methodology applied in solving the set aims and objectives.

% Depending on the project, you may like to alter the ``heading'' of this section. Check with you supervisor. Also, check what subsection or any other section that can be added in or removed from this template.

% \subsection{A subsection 1}
% \label{sec:intro_some_sub1}
% You may or may not need subsections here. Depending on your project's needs, add two or more subsection(s). A section takes at least two subsections. 

% \subsection{A subsection 2}
% \label{sec:intro_some_sub2}
% Depending on your project's needs, add more section(s) and subsection(s).

% \subsubsection{A subsection 1 of a subsection}
% \label{sec:intro_some_subsub1}
% The command \textbackslash subsubsection\{\} creates a paragraph heading in \LaTeX.

% \subsubsection{A subsection 2 of a subsection}
% \label{sec:intro_some_subsub2}
% Write your text here...

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Summary of contributions and achievements} %  use this section 
% \label{sec:intro_sum_results} % label of summary of results
% Describe clearly what you have done/created/achieved and what the major results and their implications are. 


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Organization of the report} %  use this section
% \label{sec:intro_org} % label of Org section
% Describe the outline of the rest of the report here. Let the reader know what to expect ahead in the report. Describe how you have organized your report. 

% \textbf{Example: how to refer a chapter, section, subsection}. This report is organised into seven chapters. Chapter~\ref{ch:lit_rev} details the literature review of this project. In Section~\ref{ch:method}...  % and so on.

% \textbf{Note:}  Take care of the word like ``Chapter,'' ``Section,'' ``Figure'' etc. before the \LaTeX~command \verb|ref{}|. Otherwise, a  sentence will be confusing. For example, In \ref{ch:lit_rev} literature review is described. In this sentence, the word ``Chapter'' is missing. Therefore, a reader would not know whether 2 is for a Chapter or a Section or a Figure.  For more information on \textbf{automated tools} to assist in this work, see \Cref{subsec:reftools}.

