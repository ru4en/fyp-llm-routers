\chapter{Literature Review}
\label{ch:lit_rev} %Label of the chapter lit rev. The key ``ch:lit_rev'' can be used with command \ref{ch:lit_rev} to refer this Chapter.

\section{Large Language Models: Current Landscape}

Large scale LLMs continue to grow in parameter count and capability, intensifying the trade off between performance and computational cost. Models such as OpenAI's GPT 4 and Google's Gemini 2.5 Pro deliver top tier results, but at significantly higher inference costs often 20 to 40 times more than comparable alternatives \footnote{\url{https://help.openai.com/en/articles/7127956-how-much-does-gpt-4-cost}}. With many state of the art models being closed source (only accessible through an API), a new wave of open weight and open source models has emerged. These models make it easier for individuals and companies to self host, potentially lowering operational costs. For organisations offering inference as a service, open models are particularly advantageous not only for cost efficiency, but also for addressing privacy and security concerns associated with sending user prompts to third party providers.

\section{Multi Agent Systems and Distributed AI Architecture}

Multi agent systems (MAS) have been a subject of research and development since the 1980s. While traditional MAS research established fundamental principles by using agent communication protocols such as KQML and FIPA ACL, the emergence of Large Language Models has transformed how these systems operate in practice.

In December 2023, Mistral AI introduced Mixtral 8x7B, a model that employs a Sparse Mixture of Experts (MoE) architecture suggesting a promising approach which only activates a subset of a large model's “experts” per query. This gave them the edge over other models such as Llama 2 70B on most benchmarks where inference was 6 times faster and even \textit{“matches or outperforms GPT 3.5 on most benchmarks”} \cite{hu2024routerbenchbenchmarkmultillmrouting}. While Mixtral applies routing at the model architecture level rather than through a separate system level orchestration, it demonstrated the potential for such a middle layer. This approach highlights how advanced modular designs can enhance performance. Even though the computational requirements for inference remain high for many GPUs, it was significantly less expensive to run compared to similar sized dense models. This increased demand for performance optimisation while leveraging existing models remains the core reason to research systems that deploy sophisticated multi model and multi agent systems.

\section{Semantic Routing Mechanisms}

Several recent projects provide router like middleware to manage multi model access. One such example is \textbf{OpenRouter.ai}, which provides a model route of sorts, unified into one API that provides model inference behind an endpoint, dynamically routing requests across providers to optimise cost and availability. On the open source side, \textbf{RouteLLM} formalises LLM routing as a machine learning problem. RouteLLM learns from preference data such as chatbot arena rankings to send easy queries to cheap models and hard ones to big models. Their results show that such learned routers \textit{“can significantly reduce costs without compromising quality, with cost reductions of over 85\% on MT Bench while still achieving 95\% of GPT 4's performance”} \footnote{\url{https://lmsys.org/blog/2024-07-01-routellm}}. Another routing mechanism, Router Bench, shows promise with over 405,000 inference outcomes from representative LLMs, measuring routers on metrics such as dollar per token cost, latency, and accuracy \cite{hu2024routerbenchbenchmarkmultillmrouting}.

On the tool routing side of things, most work focuses on enabling LLMs to call tools rather than on how to choose them automatically. Landmark papers like \textbf{Toolformer} \cite{schick2023toolformerlanguagemodelsteach} demonstrate how LLMs can learn to invoke tools. At the interface level, OpenAI's \textbf{Function Calling} and “built in tools” features have begun to infer tool usage directly from user prompts. For example, “google XYZ for me” automatically triggers a web search tool without explicit selection. In parallel, \textbf{LangChain} implements lightweight embedding based matching to decide when and which tools to invoke. Despite these advances, there remains a gap in formal publications on tool routing per se, especially in live inference settings.

\section{Routing Approaches}

Whilst looking for alternatives, some of the current decision making mechanisms used by LLM services are:

\begin{itemize}
    \item \textbf{Rule based routing:} This relies on a predefined set of heuristic rules or configuration files to map incoming queries to specific LLMs or tools. For example, simple keyword matching or regular expressions might be used. A terminal tool could apply a rule such as \texttt{/bash\b/} to detect a Bash code block, then execute it in a virtual shell with appropriate safety checks. This approach offers full transparency and is reliable \footnote{\url{https://developers.liveperson.com/conversation-builder-generative-ai-routing-ai-agents-route-consumers-conversationally.html}}. Each routing decision is directly traceable to an explicit rule, making the system's behaviour predictable and explainable \footnote{\url{https://aws.amazon.com/blogs/machine-learning/multi-llm-routing-strategies-for-generative-ai-applications-on-aws/}}. However, because it relies solely on hard patterns, it often lacks contextual understanding. For example, a prompt like \textit{“Could you make me a simple Snake game in Pygame?”} may not activate a development tool if the trigger is based only on a regular expression like \texttt{/python\b/}, which searches for explicit Python code blocks.

    \item \textbf{Prompt based routing:} This involves invoking a language model with a crafted system prompt. For example: \texttt{SYSTEM: “Determine whether the following prompt <USER\_PROMPT> contains Bash. If so, return only the shell commands.”} The model's response is passed to the relevant tool or agent. If a shell command is detected, it may be executed and its output returned to the user after post processing. A simple approach for model selection is to prompt a compact but capable model, such as TinyLlama, with the query and a list of available models, and ask it to select the most appropriate one. LLM based routers benefit from broad general knowledge and the ability to process complex inputs. However, they introduce higher computational overhead, latency, and occasional unreliability, making them expensive and potentially fragile.

    \item \textbf{Similarity Clustering based Routing:} This method leverages unsupervised clustering algorithms such as K-means to group historical user queries in a semantic embedding space, thereby identifying clusters of similar requests. By operating on semantic similarity rather than rigid rules, this method affords greater contextual sensitivity, enabling more flexible task appropriate routing while retaining the predictability of cluster level performance. This method is effective if the quality of the data collected is very high \cite{varangotreille2025doingimplementingrouting}.

    \item \textbf{NLI based (zero-shot) routing:} This is the approach we will implement. It employs a pre trained Natural Language Inference model, such as BART-Large-MNLI, to perform zero shot intent classification. The prompt is treated as the premise, while tool or agent descriptions are framed as hypotheses. The tool or agent with the highest scoring hypothesis is selected. This approach requires no additional training but is sensitive to the phrasing and calibration of the hypotheses. The quality of results thus depends heavily on how well these descriptions are constructed. This gives us both the reliability of Rule based routing whilst allowing the prompt to be flexible for some level of context relation.
\end{itemize}

\section{Research Gap Analysis}

As highlighted previously, multi agent routing has been successfully implemented both as closed source (\textbf{OpenRouter.ai}) as well as in open source libraries such as \textbf{RouteLLM}. These systems effectively distribute queries across multiple language models based on their respective capabilities. However, current approaches exhibit several limitations that warrant further investigation.

First, existing routing mechanisms predominantly rely on sophisticated architectures requiring substantial computational resources. For instance in the paper, \cite{jiang2023activeretrievalaugmentedgeneration} where a transformer based models with over 1 billion parameters for their routing decisions is suggested, while commercial solutions like OpenRouter.ai utilise proprietary embedding models that necessitate dedicated GPU infrastructure. These resource intensive requirements create significant barriers to deployment in resource constrained environments or edge computing scenarios.

Second, the dominant routing paradigms typically demand extensive training data encompassing diverse query model pairs with performance metrics. This data acquisition process is both time consuming and expensive, often requiring thousands of labeled examples to achieve acceptable routing accuracy. Such data requirements impede rapid adaptation to newly released language models or specialised domain applications where labeled data is scarce.

Third, while preliminary research has begun exploring NLI models for routing tasks, there remains a significant knowledge gap regarding their efficacy in production environments. The potential of NLI models specifically their ability to determine semantic relationships between user queries and model capability descriptions has not been thoroughly examined in the context of multi agent routing systems.

This research aims to address these gaps by investigating the viability of lightweight, pre trained NLI models as efficient routing mechanisms. 